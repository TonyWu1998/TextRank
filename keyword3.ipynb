{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import re\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp38-cp38-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 19.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /Users/tony/opt/anaconda3/envs/cs447/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.1.2-py3-none-any.whl (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 43.4 MB/s \n",
      "\u001b[?25hCollecting scipy>=0.18.1\n",
      "  Downloading scipy-1.6.0-cp38-cp38-macosx_10_9_x86_64.whl (30.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.7 MB 30.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /Users/tony/opt/anaconda3/envs/cs447/lib/python3.8/site-packages (from gensim) (1.19.1)\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "Successfully installed gensim-3.8.3 scipy-1.6.0 smart-open-4.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c81c1a3de974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText8Corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphrases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPhraser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_retokenizer(doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    patterns = [[{\"LOWER\": \"el\"}, {\"LOWER\": \"paso\"}]]\n",
    "    matcher.add(\"TO_MERGE\", None, *patterns)\n",
    "    matches = matcher(doc)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            retokenizer.merge(span)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(file_dir):\n",
    "    temp = ''\n",
    "    with open(file_dir, \"r\") as i:\n",
    "        for w in i:\n",
    "            if (w == \"\\n\"):\n",
    "                continue\n",
    "            else:\n",
    "                temp += w\n",
    "    return temp.splitlines()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"/Users/tony/Desktop/TextRank/oxford/Oxford Dictionary of Computer Science (7 ed.).txt\"\n",
    "oxf_vocab = split_word(file_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6511"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(oxf_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import re\n",
    "temp = \"We introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence Framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens.\"\n",
    "\n",
    "def contains_word(s, w):\n",
    "    return (' ' + w + ' ') in (' ' + s + ' ')\n",
    "\n",
    "'''\n",
    "for i in range(len(oxf_vocab)):\n",
    "    if contains_word(temp, oxf_vocab[i]):\n",
    "        print(oxf_vocab[i])\n",
    "'''\n",
    "\n",
    "output_str = re.sub('[^A-Za-z0-9]+', ' ', temp)\n",
    "\n",
    "contains_word(output_str, \"neural network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(custom_retokenizer, before=\"tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRankForKeyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        \n",
    "        '''\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                #print(\"each token: \" + str(type(token)))\n",
    "                #print(\"each token text: \" + str(type(token.text)))\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            print(selected_words)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        sentences = []\n",
    "       \n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            print(sent.text)\n",
    "            for token in sent:\n",
    "                if token.text in oxf_vocab:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            print(\"selected_words: \\n\")\n",
    "            print(selected_words)\n",
    "            sentences.append(selected_words)   \n",
    "        '''\n",
    "        #for line in text.split('\\n'):\n",
    "\n",
    "        sentences = []\n",
    "        a = 0\n",
    "        for sent in doc.split('.'):\n",
    "            if (a%5000 == 0):\n",
    "                print(\"Processed sentences: \" + str(a))\n",
    "\n",
    "            a+=1\n",
    "\n",
    "            selected_words = []\n",
    "            text = sent.lower()\n",
    "            token_text = word_tokenize(text)\n",
    "            Pos_tag = nltk.pos_tag(token_text)\n",
    "            lemmatized_text = self.lemmatize(Pos_tag)\n",
    "            stopwords_plus = self.stopwords_list(Pos_tag)\n",
    "            phrases = self.phrase_list(lemmatized_text, stopwords_plus)\n",
    "            \n",
    "            for i in phrases:\n",
    "                temp = \"\"\n",
    "                for j in i:\n",
    "                    temp += (j + ' ')\n",
    "                #print(\"temp: \" + temp)\n",
    "                for word in oxf_vocab:\n",
    "                    if contains_word(temp, word):\n",
    "                        selected_words.append(word)\n",
    "            #print(\"selected_words: \\n\")\n",
    "            #print(selected_words)\n",
    "            sentences.append(selected_words)\n",
    "\n",
    "        #print(\"sentences: \\n\")\n",
    "        #print(sentences)\n",
    "          \n",
    "        return sentences\n",
    "    \n",
    "     \n",
    "    \n",
    "\n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "    \n",
    "    # formula to calculate the weights of nodes\n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal()) \n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        d = {}\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            d[key] = value\n",
    "            #print(d)\n",
    "            if i > number:\n",
    "                break\n",
    "            \n",
    "        return d\n",
    "\n",
    "    def lemmatize(self, POS_tag):\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        adjective_tags = ['JJ','JJR','JJS']\n",
    "\n",
    "        lemmatized_text = []\n",
    "\n",
    "        for word in POS_tag:\n",
    "            if word[1] in adjective_tags:\n",
    "                lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "            else:\n",
    "                lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0])))\n",
    "        return lemmatized_text\n",
    "    \n",
    "    def stopwords_list(self, POS_tag):\n",
    "        stopwords = []\n",
    "        #POS_tag = nltk.pos_tag(lemmatized_text)POS_tag = nltk.pos_tag(lemmatized_text)\n",
    "        wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW'] \n",
    "\n",
    "        for word in POS_tag:\n",
    "            if word[1] not in wanted_POS:\n",
    "                stopwords.append(word[0])\n",
    "\n",
    "        punctuations = list(str(string.punctuation))\n",
    "\n",
    "        stopwords = stopwords + punctuations\n",
    "\n",
    "        stopword_file = open(\"long_stopwords.txt\", \"r\")\n",
    "        #Source = https://www.ranks.nl/stopwords\n",
    "\n",
    "        lots_of_stopwords = []\n",
    "\n",
    "        for line in stopword_file.readlines():\n",
    "            lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "        stopwords_plus = []\n",
    "        stopwords_plus = stopwords + lots_of_stopwords\n",
    "        stopwords_plus = set(stopwords_plus)\n",
    "        return stopwords_plus\n",
    "\n",
    "    def phrase_list(self, lemmatized_text, stopwords_plus):\n",
    "        phrases = []\n",
    "        phrase = \" \"\n",
    "        for word in lemmatized_text:\n",
    "            \n",
    "            if word in stopwords_plus:\n",
    "                if phrase!= \" \":\n",
    "                    phrases.append(str(phrase).strip().split())\n",
    "                phrase = \" \"\n",
    "            elif word not in stopwords_plus:\n",
    "                phrase+=str(word)\n",
    "                phrase+=\" \"\n",
    "\n",
    "        #print(\"Partitioned Phrases (Candidate Keyphrases): \\n\")\n",
    "        #print(phrases)\n",
    "        return phrases\n",
    "\n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'ADJ'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        #self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        \n",
    "\n",
    "        '''\n",
    "        text = text.lower()\n",
    "        token_text = word_tokenize(text)\n",
    "        \n",
    "        Pos_tag = nltk.pos_tag(token_text)\n",
    "        lemmatized_text = self.lemmatize(Pos_tag)\n",
    "        stopwords_plus = self.stopwords_list(Pos_tag)\n",
    "        phrases = self.phrase_list(lemmatized_text, stopwords_plus)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                print(token.text)\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        for sent in doc.sents:\n",
    "            displacy.render(sent, style=\"dep\")    \n",
    "        for sent in doc.sents:\n",
    "            displacy.render(sent, style=\"ent\")\n",
    "        '''\n",
    "\n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(text, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "\n",
    "        # print(token_pairs)\n",
    "\n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initialization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"arxivData.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(data)):\n",
    "    corpus.append(data[i][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "41000"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"arxivData.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "corpus = \"\"\n",
    "for i in range(len(data)):\n",
    "    corpus += data[i][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Processed sentences: 0\nProcessed sentences: 5000\nProcessed sentences: 10000\nProcessed sentences: 15000\nProcessed sentences: 20000\nProcessed sentences: 25000\nProcessed sentences: 30000\nProcessed sentences: 35000\nProcessed sentences: 40000\nProcessed sentences: 45000\nProcessed sentences: 50000\nProcessed sentences: 55000\nProcessed sentences: 60000\nProcessed sentences: 65000\nProcessed sentences: 70000\nProcessed sentences: 75000\nProcessed sentences: 80000\nProcessed sentences: 85000\nProcessed sentences: 90000\nProcessed sentences: 95000\nProcessed sentences: 100000\nProcessed sentences: 105000\nProcessed sentences: 110000\nProcessed sentences: 115000\nProcessed sentences: 120000\nProcessed sentences: 125000\nProcessed sentences: 130000\nProcessed sentences: 135000\nProcessed sentences: 140000\nProcessed sentences: 145000\nProcessed sentences: 150000\nProcessed sentences: 155000\nProcessed sentences: 160000\nProcessed sentences: 165000\nProcessed sentences: 170000\nProcessed sentences: 175000\nProcessed sentences: 180000\nProcessed sentences: 185000\nProcessed sentences: 190000\nProcessed sentences: 195000\nProcessed sentences: 200000\nProcessed sentences: 205000\nProcessed sentences: 210000\nProcessed sentences: 215000\nProcessed sentences: 220000\nProcessed sentences: 225000\nProcessed sentences: 230000\nProcessed sentences: 235000\nProcessed sentences: 240000\nProcessed sentences: 245000\nProcessed sentences: 250000\nProcessed sentences: 255000\nProcessed sentences: 260000\nProcessed sentences: 265000\nProcessed sentences: 270000\nProcessed sentences: 275000\nProcessed sentences: 280000\nProcessed sentences: 285000\nProcessed sentences: 290000\n"
    }
   ],
   "source": [
    "\n",
    "text = '''\n",
    "We introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens. There are many ways to estimate or\\nlearn the high-level coarse tokens, but we argue that a simple extraction\\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\\nSuch procedure allows training the multiresolution recurrent neural network by\\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\\nthe standard log- likelihood objective w.r.t. natural language tokens (word\\nperplexity), optimizing the joint log-likelihood biases the model towards\\nmodeling high-level abstractions. We apply the proposed model to the task of\\ndialogue response generation in two challenging domains: the Ubuntu technical\\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\\ncompeting approaches by a substantial margin, achieving state-of-the-art\\nresults according to both automatic evaluation metrics and a human evaluation\\nstudy. On Twitter, the model appears to generate more relevant and on-topic\\nresponses according to automatic evaluation metrics. Finally, our experiments\\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\\nnatural language and is better able to capture long-term structure.\n",
    "\n",
    "'''\n",
    "\n",
    "tr4w = TextRankForKeyword()\n",
    "tr4w.analyze(corpus, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "model - 13.125795771889857\nsystem - 12.383422221706004\nalgorithm - 12.34557216883533\ndata - 11.759797269491274\nnetwork - 11.267566772090648\nimage - 9.857292531626232\nlearning - 9.659608261164037\nset - 8.644454337444229\napplication - 8.286443898671838\ntask - 8.108037845394637\nfunction - 7.918245903983683\nframework - 7.820828290510847\nlanguage - 7.812459496466648\nprocess - 7.533624741655914\nstructure - 7.4050061126595494\nrepresentation - 7.393812622414277\norder - 7.193396733553941\nneural network - 6.717603676881666\npoint - 6.68934553183287\nterm - 6.684352251587636\ninput - 6.626912635113543\nmachine - 6.566161351356564\ntype - 6.420493010447493\nclass - 6.41353335900859\nparameter - 6.328547470326026\ndomain - 6.308883526128389\ndistribution - 6.186170018838922\nobject - 6.1582653523345545\npattern - 6.119195870922046\nknowledge - 6.073562089584464\narchitecture - 6.0407433324245305\naccuracy - 6.023945502174273\nsearch - 6.003048961976546\nform - 5.962380274010048\ndynamic - 5.90426024064353\nlevel - 5.8922028214680555\ngraph - 5.8850712876284845\ninference - 5.871935983245177\noptimization - 5.838960405683465\nconstraint - 5.7977364537734655\nstandard - 5.784059500723471\nprogram - 5.703637922501057\nerror - 5.67990082992662\nlocal - 5.673605889614781\nprobability - 5.602172408047508\nword - 5.601347767284428\ntheory - 5.585625611139063\ncomputer - 5.544927081765731\ntool - 5.5126103433981495\nsignal - 5.498758067499879\nmemory - 5.49864247622536\ncomplexity - 5.493197726588009\nimplementation - 5.433803349631049\nkey - 5.427167169807047\nmatrix - 5.416978905838874\nvariable - 5.410998318757756\nfield - 5.338071413126273\nlogic - 5.32340376855485\nsequence - 5.32157930167981\nvector - 5.316125924036899\nagent - 5.232685550487693\nvideo - 5.159642691055245\noutput - 5.08767387918597\ntree - 5.058282949838516\ngroup - 5.00179203304806\nlayer - 4.9903994864952805\nrate - 4.970463426811986\nnode - 4.9448584233629385\nmeasure - 4.873123065783342\nprocedure - 4.851221562318069\nrange - 4.816653622764503\nmap - 4.79984506023562\ncombination - 4.794465826685322\nrelation - 4.790994236930413\noperation - 4.786569853550297\nenvironment - 4.758732554915456\ninteraction - 4.667618031741369\nonline - 4.578344754471422\ndatabase - 4.5708820291798045\nglobal - 4.535230826592051\nmapping - 4.526130349724082\nbound - 4.525494433502538\ninstance - 4.513216097174153\nparallel - 4.512427808750555\nnoise - 4.402224697733569\nprogramming - 4.395538710033942\nmachine learning - 4.36975577928021\nclustering - 4.366470173301448\nkernel - 4.3224449317585165\nextension - 4.3186313999147705\ncode - 4.310881574699474\nlabel - 4.302053810531341\noperator - 4.297556495746542\ndocument - 4.272315557481651\nview - 4.263005972881781\nmetric - 4.2619667948176705\nsoftware - 4.183695828487483\naspect - 4.160501536255081\nsimulation - 4.15504420782437\ncharacteristic - 4.152096576948082\nsampling - 4.079931644483472\nsubset - 4.076511409542243\nbenchmark - 4.068832435370339\nresource - 4.036853745444143\nvariation - 4.032780056442941\ndevice - 3.9879596354769\nphase - 3.9578794977954352\nsensor - 3.949962517450439\ncluster - 3.9407784885175126\nmatching - 3.929318458069087\ntransformation - 3.923915558056673\nmethodology - 3.9190649250208542\nelement - 3.912584943245332\nsemantics - 3.8670383545370566\nopen - 3.8539024083198794\nframe - 3.8473329866038832\npath - 3.801816327133047\nfilter - 3.7985010131860872\nedge - 3.795578806731333\nresolution - 3.7933625187216458\ndimension - 3.7896578513747188\nidentification - 3.766808393713073\ncategory - 3.766515248402715\ndigital - 3.7633056602495634\nchoice - 3.760058828114483\nneuron - 3.758363079695086\ndegree - 3.7549749108430346\naccount - 3.7369262888724317\ntranslation - 3.736148985644968\nembedding - 3.728109119692044\nexpression - 3.719265949573023\nconvergence - 3.7111136380461147\ninterpretation - 3.7107725753784653\npixel - 3.707621303882558\naccess - 3.681990341072754\nheuristic - 3.6774437190488602\nsentence - 3.645257087216454\nattribute - 3.642476312362954\ntesting - 3.6184352638252046\nconditional - 3.6019131569902845\ncore - 3.5923498844957424\ncorrelation - 3.5887309097871523\nbase - 3.5687140568300157\nannotation - 3.5672831740138573\nrobustness - 3.5390819844580848\nblock - 3.527893357607349\nuncertainty - 3.5216542846693275\nplatform - 3.517484962937475\nlocation - 3.5067260358865133\nspeed - 3.497078660016836\nencoding - 3.4904894468937573\nprojection - 3.488106541015828\ndepth - 3.480961496100259\nlength - 3.4480158203929356\nfrequency - 3.414951696668272\ndensity - 3.4133330384000433\nmodule - 3.405769733399934\nentity - 3.384457673675265\nsequential - 3.3815175223956078\nleading - 3.3800503825508192\ndecomposition - 3.3713685133264972\nactive - 3.3432232287261345\nplanning - 3.336585088627786\ndictionary - 3.3230282758485012\ndata set - 3.2864191818494257\ndefinition - 3.2835380733719086\nprecision - 3.2808782861023764\ngeneric - 3.262741585770965\ncharacter - 3.258479972083291\ninfluence - 3.2565060820941265\nconsistency - 3.2010786433460043\ngrammar - 3.174698210653902\nconfiguration - 3.1580981091969784\nparadigm - 3.155448981007492\ncompression - 3.119138020708738\nrisk - 3.11009652465992\npopulation - 3.093144633421827\nsense - 3.076398636353232\nphysical - 3.0762372454003017\nlink - 3.075370295468787\nontology - 3.066053315405226\nrank - 3.0592490486938497\nchannel - 3.055478829003844\nfiltering - 3.0544869768133265\nsegment - 3.0462231174497334\nhardware - 3.042475070230392\nproof - 3.0360382605295584\ncoding - 3.03523826278735\nlikelihood - 3.0335326251558725\nvariance - 3.0287101945054355\nscalable - 3.028495988099161\nchain - 3.0189260321615023\ninvariant - 3.017801489039824\nmodelling - 3.0157764836597045\npolynomial - 3.003294146251135\ndescriptor - 3.003043793871481\ninterface - 2.9984399961116703\ncell - 2.97965339745912\nbias - 2.9609224706231347\nvolume - 2.958783244641102\nmessage - 2.944711516127789\nconstant - 2.9375502584519477\nhierarchy - 2.9297568012535598\niteration - 2.9100656351221246\nstatic - 2.900195393976888\nfeedback - 2.8960685541795104\nspecification - 2.896057496375313\nminimization - 2.867239774899946\nlist - 2.8627792817921196\ncomposition - 2.862418522857716\nentropy - 2.856826702368891\ncapacity - 2.8509148504391373\naddress - 2.848638192911392\nstream - 2.838525037702963\nreinforcement learning - 2.832427040140187\nparsing - 2.826643656211095\nequation - 2.820482994272794\nbody - 2.8198684233964486\nclose - 2.813144409634996\nlocalization - 2.801305582411654\nengine - 2.7945288163272712\nquantum - 2.786062364558912\nshare - 2.769680852555215\nprotocol - 2.749595797891514\nscaling - 2.7484128615906394\nlogical - 2.7445651621059577\nmedium - 2.7175148758347327\ntime series - 2.715168567494107\ntraffic - 2.703025789498762\ndeterministic - 2.672659489174971\npatch - 2.6712940332722286\nnorm - 2.6692262088738437\ninverse - 2.655859835152365\nsecurity - 2.6438552475189954\nconvolution - 2.634897392906317\ngrid - 2.6262590890948614\nreview - 2.6198729123472124\nrunning - 2.6186471763957897\nmode - 2.595674622353034\nrestriction - 2.59492422201144\ngenerator - 2.5718885486695644\ncircuit - 2.5665312521356705\ndual - 2.5665012884641945\nforest - 2.555752573434801\nstability - 2.554840036992892\nstorage - 2.539647257383665\npartition - 2.5175582088679724\nargument - 2.5056329114522367\ntable - 2.4989752562655587\nlibrary - 2.4985902117434637\nshift - 2.4983983287997567\nintensity - 2.4964010382155735\nrecovery - 2.4841308727755\ninteractive - 2.468020855087328\nverification - 2.4623045382097133\nresidual - 2.4530118815671402\ngenetic algorithm - 2.426763740210368\ntopology - 2.422642254185495\nwindow - 2.390225297930539\nnormalization - 2.3896475264305232\nartificial intelligence - 2.3794561605761517\nprototype - 2.378531526279456\nbit - 2.371535129946832\ncovariance - 2.3669148768909736\nconstruct - 2.345774128289832\narray - 2.3232677270921407\ntemplate - 2.3215902577876246\ncontroller - 2.319682604193873\ncycle - 2.313839102835252\nperception - 2.3076633006702347\nclear - 2.2880623549570536\nvalidation - 2.288020130912754\nsign - 2.266361434639561\noutlier - 2.2624071933040533\nsymbol - 2.256958314359655\nscalability - 2.251393725160606\nvertex - 2.245143392939241\nreliability - 2.2434078338530368\nrecursive - 2.240088261106805\nrecord - 2.233401888953635\nabstraction - 2.22842033594021\nfind - 2.227686514298288\nconjunction - 2.2253570583483286\nexpectation - 2.218463214825799\nvisualization - 2.2156192884430435\nfailure - 2.2151862837287073\nhistogram - 2.2102150482257414\nnetwork architecture - 2.2077956982040465\ncovering - 2.2005338009762414\nstring - 2.192572526011294\nexpansion - 2.1882194711551604\nsignature - 2.185580609626905\nalgebra - 2.168381165583074\nconnectivity - 2.1634648979771463\nwavelet - 2.149278094018327\nsearching - 2.141883338768994\nprimitive - 2.1349129455658122\ndecoding - 2.1261688622774577\ndecoder - 2.120904631710758\nspeech recognition - 2.1152957346269616\nhead - 2.099486333037128\nquantization - 2.0896198663744365\nbalance - 2.054501972110251\nautomaton - 2.031596218321428\nsafety - 2.026415737377745\nequivalence - 2.026027300425864\nperiod - 2.0222779821520973\nknowledge base - 2.01710206333371\nimage processing - 2.011812930400914\nbranch - 2.011315600610798\nparser - 2.009713042725062\nrefinement - 1.9960918874469478\nprivacy - 1.9951234680560894\ninitialization - 1.994459024967997\nencoder - 1.987299880564151\nscan - 1.9864552829966295\nindicator - 1.9814221938926808\nfraction - 1.9758646625073628\ntrack - 1.9684533287139705\nbottleneck - 1.9664757421566978\nreturn - 1.958820603316651\ndata mining - 1.9568595430113773\nschema - 1.9531522600087787\naddressing - 1.9427891905649055\navailability - 1.940703104280061\nmachine translation - 1.937617943019485\narm - 1.9374188215327819\ninteger - 1.937372423759835\ntag - 1.9106687041258077\nformat - 1.8994279361686823\nloop - 1.894442520320466\nasymptotic - 1.8835898367290125\nerror rate - 1.881738964763937\nupper bound - 1.8747035549693165\nsyntax - 1.8707777867286408\nroot - 1.870703331858437\nprocessor - 1.8690330201466416\nillumination - 1.8646696247352068\nredundancy - 1.856773365788968\nmerging - 1.8558597780295556\npattern recognition - 1.8549184101195575\nelectronic - 1.8533226405330883\ninterpolation - 1.846947598289146\nproduction - 1.8462930805018685\ncost function - 1.8300928796555125\nenable - 1.8225243412170848\nhashing - 1.8225146012309508\ndecision tree - 1.8200038140712715\noffline - 1.8184348349898927\nsmoothing - 1.8076057152487457\nlattice - 1.8070436954653375\nrandom variable - 1.7989675112632484\ncall - 1.7975582900662435\nscope - 1.7828108799421827\ngate - 1.7743330648942401\nconsensus - 1.7704349004350886\nstatement - 1.7641095146392307\npicture - 1.760178824736517\nrendering - 1.7419102644638524\nstate space - 1.739305706504262\nderivative - 1.7378955253855515\ncoupling - 1.7273283791623157\nreader - 1.7094322251714602\nrobotics - 1.704456834965098\nserver - 1.7026082719593196\nsplit - 1.6968722120067352\nrouting - 1.6863502108090187\ndefault - 1.6852731325142876\ngesture - 1.6803537324239097\nremote - 1.6797327238157238\ntoken - 1.676931215483525\nleaf - 1.6750414745907327\nfile - 1.6577482706821889\nintersection - 1.6551510332894566\ntracker - 1.6506500449771662\ninduction - 1.6493939112660851\nmember - 1.6467384180113962\nsimulator - 1.6448941596606836\nprincipal component analysis - 1.6424748788289074\noptical flow - 1.6407399540269205\nstreaming - 1.6398202151291903\npermutation - 1.6367639383522676\nhighlight - 1.63589977407262\naxiom - 1.6123269408430345\nartificial neural network - 1.6080993845561529\natom - 1.6074888761997204\nunion - 1.6063756922570493\nperceptron - 1.5996844663907068\nlandscape - 1.5993844576872043\ncascade - 1.5846593450287978\nfault - 1.578525258005297\npackage - 1.5702488468450793\nclosed - 1.5658334009734771\ndata structure - 1.5587704568798921\nhandle - 1.5568612789009881\nbridge - 1.5488048397994274\ninstruction - 1.5385280590157215\nletter - 1.5350291091449297\nasynchronous - 1.523576440979241\nmesh - 1.5168033207652099\nstack - 1.5151928149038594\ninequality - 1.5124344277494228\nminimax - 1.4979486483195754\nwidth - 1.4911044614912718\nband - 1.491080942728808\npredicate - 1.4870008127958023\nchild - 1.4820811030887509\npolynomial time - 1.4769906013703507\noverlap - 1.4639311241845032\nviewing - 1.4611539376803644\ncut - 1.4565007186677397\nrepository - 1.4506463254402284\nknowledge representation - 1.4482425388667708\ncomplement - 1.4448031916648145\nswitching - 1.440855326194583\nspread - 1.4404328536070343\nsatisfiability - 1.4314777706578883\nsuite - 1.4234084472018775\njob - 1.4202629541859206\ndisplay - 1.4173872730667303\ncurvature - 1.416422513726478\nscript - 1.4143877070928368\ngranularity - 1.4140148590231614\nwebsite - 1.4126561689370627\nsignal processing - 1.4109645691512314\nstore - 1.4062814257512972\nlatency - 1.3991864269573604\nwireless - 1.3888649527946346\nmedian - 1.382706100359564\nbandwidth - 1.380914544322764\nbag - 1.37776978819331\nstatus - 1.3765878194885959\ncomputer science - 1.3739322454002978\ncommand - 1.369591494631241\ncomment - 1.361853509398165\ntime complexity - 1.3616534281226982\ndrive - 1.3591797469034614\nnegation - 1.3577807168336564\ndeveloper - 1.354196219189062\nmarker - 1.3476888640313707\ninstantiation - 1.3441364008462044\nroute - 1.3427604253235526\ndynamic programming - 1.3421804265748825\nopen-source - 1.3417450921358842\nwarping - 1.337413622959426\nclosure - 1.3345005460809376\nslice - 1.3117513850276343\ntransport - 1.3084161898417408\nthroughput - 1.3031753027966104\ndomain knowledge - 1.2936543313622693\ndiscriminant analysis - 1.2926094690218473\ndriver - 1.2887606657186206\nmultiplier - 1.2884161342829779\ncomputable - 1.2878860865626254\nhighlighting - 1.2773534667737891\nroutine - 1.2763052509137807\nphoneme - 1.2678654046778122\nfuzzy logic - 1.259820843326759\ninstrument - 1.2591016929969254\ncopy - 1.2510924975204532\ngenetic programming - 1.2507765623283915\ntransparent - 1.2492665904532343\nmonotonic - 1.2405367240971992\ndata collection - 1.2355924226417425\ncardinality - 1.2343992312170915\nmultimedia - 1.2342027782098737\ncompatibility - 1.2316411108829075\nproxy - 1.2310009171598741\nfinite set - 1.2284337246765\nclient - 1.2164477372535667\ncompleteness - 1.1993536579016562\nprogramming language - 1.1966936951128226\ndecision problem - 1.1955442169675985\nnarrow - 1.1953313696698982\ndiscretization - 1.1883888825355138\nstatistical analysis - 1.1852029175181018\nnull - 1.1804422414688287\nmaintenance - 1.1801872220886653\nserial - 1.1790619732317982\ntoolbox - 1.1786232612571936\nmalware - 1.1692433017142325\narc - 1.166738868792554\nvoxel - 1.16410579774573\nscalar - 1.1598409929180606\ninconsistent - 1.1588229715147484\nbioinformatics - 1.155569212022448\ntail - 1.1547464085784518\ncheck - 1.1485294682785452\nfractal - 1.1462936862624042\nsynchronous - 1.1445283584443495\namplitude - 1.1432065066634305\nfeed - 1.1422690232331791\ndisjoint - 1.1410712473095237\nblack box - 1.1391227154334065\nsorting - 1.1357828706619062\nfilling - 1.1346031084443138\nliteral - 1.1290606891922006\nconstraint satisfaction - 1.1287548301476686\nremote sensing - 1.1267294286052218\nsource code - 1.1099212510164826\nlaser - 1.1049273097871417\nthreat - 1.1008516451550618\n"
    }
   ],
   "source": [
    "dict_rank = tr4w.get_keywords(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "502"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "len(dict_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '''\n",
    "We introduce the multiresolution recurrent neural network, which extends the\\nsequence-to-sequence framework to model natural language generation as two\\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\\nand a sequence of natural language tokens. There are many ways to estimate or\\nlearn the high-level coarse tokens, but we argue that a simple extraction\\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\\nSuch procedure allows training the multiresolution recurrent neural network by\\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\\nthe standard log- likelihood objective w.r.t. natural language tokens (word\\nperplexity), optimizing the joint log-likelihood biases the model towards\\nmodeling high-level abstractions. We apply the proposed model to the task of\\ndialogue response generation in two challenging domains: the Ubuntu technical\\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\\ncompeting approaches by a substantial margin, achieving state-of-the-art\\nresults according to both automatic evaluation metrics and a human evaluation\\nstudy. On Twitter, the model appears to generate more relevant and on-topic\\nresponses according to automatic evaluation metrics. Finally, our experiments\\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\\nnatural language and is better able to capture long-term structure.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "line: \nWe introduce the multiresolution recurrent neural network, which extends the\nsequence-to-sequence framework to model natural language generation as two\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\nand a sequence of natural language tokens\nline:  There are many ways to estimate or\nlearn the high-level coarse tokens, but we argue that a simple extraction\nprocedure is sufficient to capture a wealth of high-level discourse semantics\nline: \nSuch procedure allows training the multiresolution recurrent neural network by\nmaximizing the exact joint log-likelihood over both sequences\nline:  In contrast to\nthe standard log- likelihood objective w\nline: r\nline: t\nline:  natural language tokens (word\nperplexity), optimizing the joint log-likelihood biases the model towards\nmodeling high-level abstractions\nline:  We apply the proposed model to the task of\ndialogue response generation in two challenging domains: the Ubuntu technical\nsupport domain, and Twitter conversations\nline:  On Ubuntu, the model outperforms\ncompeting approaches by a substantial margin, achieving state-of-the-art\nresults according to both automatic evaluation metrics and a human evaluation\nstudy\nline:  On Twitter, the model appears to generate more relevant and on-topic\nresponses according to automatic evaluation metrics\nline:  Finally, our experiments\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\nnatural language and is better able to capture long-term structure\nline: \n\n"
    }
   ],
   "source": [
    "for line in text1.split('.'):\n",
    "    print(\"line: \" + line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}